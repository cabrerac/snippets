\ifndef{latentVariableMotivation} 
\define{latentVariableMotivation}

\editme

\subsection{Latent Variables and Dimensionality Reduction}

\notes{Why does dimensionality reduction work on real data? The key insight is that while our measurements may be very high-dimensional, the underlying phenomena we're measuring often have much lower intrinsic dimensionality. For example:

1. A motion capture recording might have hundreds of coordinates, but these are all generated by a person's movements that have far fewer degrees of freedom.

2. Genetic data may record thousands of gene expression levels, but these are often controlled by a much smaller number of regulatory processes.

3. Images contain millions of pixels, but the actual meaningful content often lies on a much lower-dimensional manifold.}

\setupcode{import numpy as np
import matplotlib.pyplot as plt
import mlai.plot as plot}

\code{# Example showing how a 1D curve appears in 2D
t = np.linspace(0, 2*np.pi, 100)
x = np.column_stack([np.cos(t), np.sin(t)])

# Plot the data
fig, ax = plt.subplots(1, 2, figsize=plot.big_wide_figsize)
ax[0].plot(x[:, 0], x[:, 1], 'b.') 
ax[0].set_xlabel('$x_1$')
ax[0].set_ylabel('$x_2$')

# Plot the latent variable representation 
ax[1].plot(t, np.zeros_like(t), 'r.')
ax[1].set_xlabel('$z$')
ax[1].set_yticks([])}

\notes{This example shows how data that appears to be 2-dimensional (left) can actually be described by a single latent variable $z$ (right) that traces out the curve. The key premise of dimensionality reduction is finding and working with these simpler underlying representations.}

\endif
