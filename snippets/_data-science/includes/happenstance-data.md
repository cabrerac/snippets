\ifndef{happenstanceData}
\define{happenstanceData}

\editme

\subsection{Happenstance Data}

\notes{Following the revolution of mathematical statistics, data
became a carefully curated commodity. It was actively connected in
response to a scientific hypothesis. While different approaches to
statistical hypothesis testing have been the subject of longstanding
debates, there is no controversy around the notion that in order to
remove confounders you must have a well-designed experiment, and
randomization has been a mainstay of statistical data collection for a
century now. Randomized trials are used today more so than ever
before, in particular due to their widespread use in interface design
by large tech companies. Social experiments involving randomization
across many millions of users are trivially implementable in real
time. These A/B tests dictate our modern user experience. }

\notes{Such experiments are still carefully designed to remain valid,
but the modern data environment is not only about larger experimental
data, but perhaps more so about what I term "happenstance data". Data
that was not collected with a particular purpose in mind, but which is
simply being recorded in the normal course of events due to increasing
interconnection between portable digital devices and decreasing cost
of storage. }

\notes{Happenstance data are the breadcrumbs of our trail through the
forest of life. They may be being written for a particular purpose,
but later we wish to consume them for a different purpose. For
example, within the recent Covid-19 pandemic, the Royal Society DELVE
initiative [@Delve:economics20] was able to draw on transaction data
to give near-real time assessments on the effect of the pandemic and governmental response on GDP[^payments] (see also
@Carvalho:tracking20).

[^payments]: Although challenges with availability of payments data
    within the UK meant that the researchers were able to get good
    assessment of the Spanish and French economies, but struggled to
    assess their main target, the United Kingdom.}

\notes{Historically, data was expensive. It was carefully collected
according to a design. Statistical surveys are still expensive, but
today there is a strong temptation to do it on the cheap. To use
happenstance data to achieve what had been done in the past only
through rigorous data-fieldwork. A Professor Efron points out, early
attempts to achieve this, such as the Google flu predictor have been
somewhat naive [@Ginsberg:detecting09;@Halevy:unreasonable09], but as
these methodologies are gaining traction in the social sciences
[@Salganik:bitbybit18] and the field of Computational Social Science
[@Alvarez:computational16] emerges we can expect more innovation and
more ideas that may help us bridge the fundamentally different
characters of qualitative and quantitative research. For the moment,
one particularly promising approach is to use measures derived from
happenstance data (such as searches for flu) as proxy indicators for
statistics that are rigorously surveilled. With the Royal Society's
DELVE initiative, examples of this approach include work of Peter
Diggle to visualize the progression of the Covid-19 disease. Across
the UK the "Zoe App" has been used for self-reporting of Covid
symptoms [@Menni:tracking20], and by interconnecting this data with
Office for National Statistics surveys [@ONS:covid19infection20],
Peter has been able to calibrate the Zoe map of Covid-19 prevalence,
allowing nowcasting of the disease that was validated by the
production of ONS surveys. These enriched surveys can already be done
without innovation to our underlying mathematical }

\notes{So the statistical methodologies remain the gold-standard by
which these new methodologies should be judged. The situation reminds
me somewhat of the challenges Xerox faced with the advent of the
computer revolution. With great prescience, Xerox realized that the
advent of the computer meant that information was going to be shared
more often on screens. As a company whose main revenue stream was
coming from photocopying documents the notion of the paperless office
represented something of a threat to Xerox. Xerox famously responded
by funding their PARC research center, where many of the innovations
that underpin the modern computer were developed: the Xerox Alto (the
first graphical user interface), the laser printer, ethernet. These
inventions were commercial successes, although often for other
companies, but as they propagated there was a greater need for
paper. The computers produced more information, and much of it was
still shared on paper. Per capita paper consumption continued to rise
in the US until it peaked at around the turn of the millennium
[@Andres:internet14]. A similar story should now apply with the
advent of predictive models and data science. The increasing use of
predictive methodologies does not obviate the need for classical
statistical approaches, it makes them more important than ever
before.}

\notes{So, we may breathe easy that there is an ongoing role for the
classical methodologies we have at our disposal, and historical
precedent indicates the demand for those methodologies will likely
increase before any fading. What about new mathematical theories? How
can we come to a formalism for a new mathematical data science, just
as early 20th century statisticians were able to reformulate
statistics on a rigorous mathematical footing?}

\endif
